{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import mlflow\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Itaipu_Benchmarking_Bacia_Incremental_V1_Corrigido\")\n",
    "\n",
    "RANDOM_SEED=21\n",
    "model_ = \"linear_regression\"\n",
    "src_type = \"benchmark\"\n",
    "label = \"itaipu\"\n",
    "\n",
    "dir_results = f\"../../data/results/{src_type}\"\n",
    "dir_figures = f\"{dir_results}/figures/{model_}\"\n",
    "\n",
    "if not os.path.exists(dir_figures):\n",
    "    os.makedirs(dir_figures)\n",
    "\n",
    "path_datasets = \"../../data/datasets\"\n",
    "dataset = \"Itaipu_POC_VAZAO_V3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_constructor(df, n, f):\n",
    "    for i in range(1, n): \n",
    "        df[f'bacia_prec_sum (time - {i})'] = df['bacia_prec_sum'].shift(i)\n",
    "        df[f'vazao_itaipu (time - {i})'] = df['vazao_itaipu'].shift(i)\n",
    "\n",
    "    df['bacia_prec_sum (time)'] = df['bacia_prec_sum']\n",
    "    df['vazao_itaipu (time)'] = df['vazao_itaipu']\n",
    "\n",
    "    for i in range(1,f+1):\n",
    "        df[f'bacia_prec_sum (time + {i})'] = df['bacia_prec_sum'].shift(-i)\n",
    "        \n",
    "    df[f'vazao_itaipu (time + {f})'] = df['vazao_itaipu'].shift(-f)\n",
    "\n",
    "    df = df.drop(columns=['bacia_prec_sum','vazao_itaipu'])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(df):\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    # Columns to scale for X and y\n",
    "    columns_to_scale_X = df.columns[:-1]\n",
    "    columns_to_scale_y = df.columns[-1]\n",
    "\n",
    "    # Fit scalers on the selected columns and transform\n",
    "    scaled_data_X = scaler_X.fit_transform(df[columns_to_scale_X])\n",
    "    scaled_data_y = scaler_y.fit_transform(df[[columns_to_scale_y]])\n",
    "\n",
    "    # Create DataFrame with scaled data\n",
    "    scaled_X = pd.DataFrame(scaled_data_X, columns=columns_to_scale_X)\n",
    "    scaled_y = pd.DataFrame(scaled_data_y, columns=[columns_to_scale_y])\n",
    "\n",
    "    # Concatenate scaled columns to the original DataFrame\n",
    "    new_df = pd.concat([pd.DataFrame(df.index), scaled_X, scaled_y], axis=1)\n",
    "    new_df.set_index('time', inplace=True)\n",
    "\n",
    "    return new_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kge(evaluation, simulations):\n",
    "    \"\"\"Original Kling-Gupta Efficiency (KGE) and its three components\n",
    "    (r, α, β) as per `Gupta et al., 2009\n",
    "    <https://doi.org/10.1016/j.jhydrol.2009.08.003>`_.\n",
    "\n",
    "    Note, all four values KGE, r, α, β are returned, in this order.\n",
    "\n",
    "    :Calculation Details:\n",
    "        .. math::\n",
    "           E_{\\\\text{KGE}} = 1 - \\\\sqrt{[r - 1]^2 + [\\\\alpha - 1]^2\n",
    "           + [\\\\beta - 1]^2}\n",
    "        .. math::\n",
    "           r = \\\\frac{\\\\text{cov}(e, s)}{\\\\sigma({e}) \\\\cdot \\\\sigma(s)}\n",
    "        .. math::\n",
    "           \\\\alpha = \\\\frac{\\\\sigma(s)}{\\\\sigma(e)}\n",
    "        .. math::\n",
    "           \\\\beta = \\\\frac{\\\\mu(s)}{\\\\mu(e)}\n",
    "\n",
    "        where *e* is the *evaluation* series, *s* is (one of) the\n",
    "        *simulations* series, *cov* is the covariance, *σ* is the\n",
    "        standard deviation, and *μ* is the arithmetic mean.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate error in timing and dynamics r\n",
    "    # (Pearson's correlation coefficient)\n",
    "    sim_mean = np.mean(simulations, axis=0, dtype=np.float64)\n",
    "    obs_mean = np.mean(evaluation, dtype=np.float64)\n",
    "\n",
    "    r_num = np.sum((simulations - sim_mean) * (evaluation - obs_mean),\n",
    "                   axis=0, dtype=np.float64)\n",
    "    r_den = np.sqrt(np.sum((simulations - sim_mean) ** 2,\n",
    "                           axis=0, dtype=np.float64)\n",
    "                    * np.sum((evaluation - obs_mean) ** 2,\n",
    "                             dtype=np.float64))\n",
    "    r = r_num / r_den\n",
    "    # calculate error in spread of flow alpha\n",
    "    alpha = np.std(simulations, axis=0) / np.std(evaluation, dtype=np.float64)\n",
    "    # calculate error in volume beta (bias of mean discharge)\n",
    "    beta = (np.sum(simulations, axis=0, dtype=np.float64)\n",
    "            / np.sum(evaluation, dtype=np.float64))\n",
    "    # calculate the Kling-Gupta Efficiency KGE\n",
    "    kge_ = 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "    return kge_, r, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nse(evaluation, simulations):\n",
    "#     \"\"\"Nash-Sutcliffe Efficiency (NSE) as per `Nash and Sutcliffe, 1970\n",
    "#     <https://doi.org/10.1016/0022-1694(70)90255-6>`_.\n",
    "\n",
    "#     :Calculation Details:\n",
    "#         .. math::\n",
    "#            E_{\\\\text{NSE}} = 1 - \\\\frac{\\\\sum_{i=1}^{N}[e_{i}-s_{i}]^2}\n",
    "#            {\\\\sum_{i=1}^{N}[e_{i}-\\\\mu(e)]^2}\n",
    "\n",
    "#         where *N* is the length of the *simulations* and *evaluation*\n",
    "#         periods, *e* is the *evaluation* series, *s* is (one of) the\n",
    "#         *simulations* series, and *μ* is the arithmetic mean.\n",
    "\n",
    "#     \"\"\"\n",
    "#     nse_ = 1 - (\n",
    "#             np.sum((evaluation - simulations) ** 2, axis=0, dtype=np.float64)\n",
    "#             / np.sum((evaluation - np.mean(evaluation)) ** 2, dtype=np.float64)\n",
    "#     )\n",
    "\n",
    "#     return nse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example values for evaluation and simulation\n",
    "# evaluation = np.array([1, 2, 3, 4, 5, 6.2, 8])\n",
    "# simulations = np.array([1.1, 2.2, 3.3, 4.4, 5.5, 7, 9])\n",
    "\n",
    "# # Call the function and print the result\n",
    "# print(\"Nash-Sutcliffe Efficiency:\", nse(evaluation, simulations), r2_score(evaluation, simulations))\n",
    "\n",
    "# # For the application of NSE in regression procedures (i.e. when the total sum of squares can be partitioned into error and \n",
    "# # regression components), the Nash–Sutcliffe efficiency is equivalent to the coefficient of determination (R2), thus ranging between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    corr = np.corrcoef(y_true.ravel(), y_pred.ravel())[0, 1]\n",
    "    # nse_ = nse(y_true, y_pred)\n",
    "    kge_, kge_r, kge_alpha, kge_beta = kge(y_true, y_pred)\n",
    "\n",
    "    return rmse, mae, r2, corr, kge_, kge_r, kge_alpha, kge_beta # nse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_run(n, f, run_name, params, df):\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Logging params\n",
    "        mlflow.log_param(\"model\", model_)\n",
    "        mlflow.log_param(\"label\", label)\n",
    "        mlflow.log_param(\"n_so_retro\", n)\n",
    "        mlflow.log_param(\"f_so_pred\", f)\n",
    "        mlflow.log_param(\"seed\", RANDOM_SEED)\n",
    "        for key, value in params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Logging run_name as a tag\n",
    "        mlflow.set_tag(\"run_name\", run_name)\n",
    "\n",
    "        df_poc = dataset_constructor(df.copy(), n, f)\n",
    "\n",
    "        df_poc, scaler_y = scaling_data(df_poc)\n",
    "\n",
    "        # Prepare X and y data and apply train_test_split\n",
    "        X_data = df_poc.iloc[:,:-1].astype('float64')\n",
    "        y_data = df_poc.iloc[:,-1:].astype('float64')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "        # Mount model and fit it\n",
    "        model = LinearRegression(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Collect the scaled and unscaled predictions # _ stands for normalized data\n",
    "        y_pred_ = model.predict(X_test)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_.reshape(-1,1))\n",
    "\n",
    "        y_test_ = y_test\n",
    "        y_test = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "        rmse, mae, r2, corr, kge_, kge_r, kge_alpha, kge_beta = evaluation_metrics(y_test, y_pred)\n",
    "        \n",
    "        # signature = mlflow.models.signature.infer_signature(X_test, y_test_)\n",
    "        # mlflow.sklearn.log_model(model, \"sk_models\", signature=signature)\n",
    "\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2\", r2)  \n",
    "        mlflow.log_metric(\"corr\", corr)\n",
    "        # mlflow.log_metric(\"nse\", nse_)\n",
    "        mlflow.log_metric(\"kge\", kge_)\n",
    "        mlflow.log_metric(\"kge_r\", kge_r)\n",
    "        mlflow.log_metric(\"kge_alpha\", kge_alpha)\n",
    "        mlflow.log_metric(\"kge_beta\", kge_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Número de Semanas Operativas Retroativas a serem utilizadas no Treinamento dos Algoritmos. min(n)=1\n",
    "n = 5\n",
    "\n",
    "## Número da Semana Operativa Futura da Vazão a ser prevista pelos Modelos. min(f)=1\n",
    "f = 2\n",
    "\n",
    "params = {\n",
    "    'fit_intercept': True,\n",
    "    # 'normalize': False,\n",
    "}\n",
    "\n",
    "run_name = f\"single_{model_}_n={n}_f={f}_fitIn={params['fit_intercept']}_\"\n",
    "\n",
    "df = pd.read_csv(f'{path_datasets}/{dataset}', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run(n, f, run_name, params, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Número de Semanas Operativas Retroativas a serem utilizadas no Treinamento dos Algoritmos\n",
    "n_range = range(1,8+1)\n",
    "\n",
    "## Número da Semana Operativa Futura da Vazão a ser prevista pelos Modelos. min(f)=1\n",
    "f_range = range(1,8+1)\n",
    "\n",
    "\n",
    "params_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    # 'normalize': [True, False],\n",
    "}\n",
    "\n",
    "# Generate all possible combinations and create a list of dictionaries representing each combination\n",
    "params_combinations = list(product(*params_grid.values()))\n",
    "params_list = [dict(zip(params_grid.keys(), combination)) for combination in params_combinations]\n",
    "\n",
    "df = pd.read_csv(f'{path_datasets}/{dataset}', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_range:\n",
    "    for f in f_range:\n",
    "        for params in params_list:\n",
    "\n",
    "            run_name = (\n",
    "                f\"{model_}_n={n}_f={f}_\"\n",
    "                f\"fit_intercept={params['fit_intercept']}_\"\n",
    "            ) \n",
    "\n",
    "            # Check if the run_name already exists\n",
    "            existing_runs = mlflow.search_runs(filter_string=f\"tags.run_name='{run_name}'\")\n",
    "            if not existing_runs.empty:\n",
    "                last_run = existing_runs.iloc[0]  # Check the most recent run\n",
    "                if last_run[\"status\"] == \"FAILED\":\n",
    "                    print(f\"Run '{run_name}' failed previously. Re-running.\")\n",
    "                else:\n",
    "                    print(f\"Run '{run_name}' already exists. Skipping iteration.\")\n",
    "                    continue\n",
    "\n",
    "            mlflow_run(n, f, run_name, params, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".euros",
   "language": "python",
   "name": ".euros"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

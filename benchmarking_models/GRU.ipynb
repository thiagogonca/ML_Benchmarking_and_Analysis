{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import mlflow\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Itaipu_Benchmarking_Bacia_Incremental_V1_Corrigido\")\n",
    "\n",
    "RANDOM_SEED=21\n",
    "model_ = \"gru\"\n",
    "src_type = \"benchmark\"\n",
    "label = \"itaipu\"\n",
    "\n",
    "dir_results = f\"../../data/results/{src_type}\"\n",
    "pred_type = 'so_prev' # previsão para a semana operacional seguinte\n",
    "\n",
    "dir_rna = f'{dir_results}/rna'\n",
    "if not os.path.exists(dir_rna):\n",
    "    os.makedirs(dir_rna)\n",
    "\n",
    "file_ann = f'{dir_rna}/{model_}_{pred_type}.h5' \n",
    "best_file_ann = f'{dir_rna}/best_{model_}_{pred_type}.h5'\n",
    "\n",
    "path_datasets = \"../../data/datasets\"\n",
    "dataset = \"Itaipu_POC_VAZAO_V3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_constructor(df, f):\n",
    "    df['time'] = df.index\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df[f'bacia_prec_sum_shift_f={f}'] = df['bacia_prec_sum'].shift(-f)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps, f_pred):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+(f_pred-1) >= len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix+(f_pred-1), [0,2]]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(df, f_pred):\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    # Columns to scale for X and y\n",
    "    columns_to_scale_X = [f'bacia_prec_sum_shift_f={f_pred}']\n",
    "    columns_to_scale_y = ['vazao_itaipu']\n",
    "\n",
    "    # Fit scalers on the selected columns and transform\n",
    "    scaled_data_X = scaler_X.fit_transform(df[columns_to_scale_X])\n",
    "    scaled_data_y = scaler_y.fit_transform(df[columns_to_scale_y])\n",
    "\n",
    "    # Create DataFrame with scaled data\n",
    "    scaled_X = pd.DataFrame(scaled_data_X, columns=columns_to_scale_X)\n",
    "    scaled_y = pd.DataFrame(scaled_data_y, columns=columns_to_scale_y)\n",
    "\n",
    "    # Concatenate scaled columns to the original DataFrame\n",
    "    new_df = pd.concat([df.time, scaled_X, scaled_y], axis=1)\n",
    "\n",
    "    return new_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kge(evaluation, simulations):\n",
    "    \"\"\"Original Kling-Gupta Efficiency (KGE) and its three components\n",
    "    (r, α, β) as per `Gupta et al., 2009\n",
    "    <https://doi.org/10.1016/j.jhydrol.2009.08.003>`_.\n",
    "\n",
    "    Note, all four values KGE, r, α, β are returned, in this order.\n",
    "\n",
    "    :Calculation Details:\n",
    "        .. math::\n",
    "           E_{\\\\text{KGE}} = 1 - \\\\sqrt{[r - 1]^2 + [\\\\alpha - 1]^2\n",
    "           + [\\\\beta - 1]^2}\n",
    "        .. math::\n",
    "           r = \\\\frac{\\\\text{cov}(e, s)}{\\\\sigma({e}) \\\\cdot \\\\sigma(s)}\n",
    "        .. math::\n",
    "           \\\\alpha = \\\\frac{\\\\sigma(s)}{\\\\sigma(e)}\n",
    "        .. math::\n",
    "           \\\\beta = \\\\frac{\\\\mu(s)}{\\\\mu(e)}\n",
    "\n",
    "        where *e* is the *evaluation* series, *s* is (one of) the\n",
    "        *simulations* series, *cov* is the covariance, *σ* is the\n",
    "        standard deviation, and *μ* is the arithmetic mean.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate error in timing and dynamics r\n",
    "    # (Pearson's correlation coefficient)\n",
    "    sim_mean = np.mean(simulations, axis=0, dtype=np.float64)\n",
    "    obs_mean = np.mean(evaluation, dtype=np.float64)\n",
    "\n",
    "    r_num = np.sum((simulations - sim_mean) * (evaluation - obs_mean),\n",
    "                   axis=0, dtype=np.float64)\n",
    "    r_den = np.sqrt(np.sum((simulations - sim_mean) ** 2,\n",
    "                           axis=0, dtype=np.float64)\n",
    "                    * np.sum((evaluation - obs_mean) ** 2,\n",
    "                             dtype=np.float64))\n",
    "    r = r_num / r_den\n",
    "    # calculate error in spread of flow alpha\n",
    "    alpha = np.std(simulations, axis=0) / np.std(evaluation, dtype=np.float64)\n",
    "    # calculate error in volume beta (bias of mean discharge)\n",
    "    beta = (np.sum(simulations, axis=0, dtype=np.float64)\n",
    "            / np.sum(evaluation, dtype=np.float64))\n",
    "    # calculate the Kling-Gupta Efficiency KGE\n",
    "    kge_ = 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "    return kge_, r, alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    corr = np.corrcoef(y_true.ravel(), y_pred.ravel())[0, 1]\n",
    "    # nse_ = nse(y_true, y_pred)\n",
    "    kge_, kge_r, kge_alpha, kge_beta = kge(y_true, y_pred)\n",
    "\n",
    "    return rmse, mae, r2, corr, kge_, kge_r, kge_alpha, kge_beta # nse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neurons_hl = [128, 123, 246, 125]\n",
    "# for idx, n_neurons in enumerate(n_neurons_hl):\n",
    "#     print(idx, n_neurons) if n_neurons % 2 != 0 else None\n",
    "#     return_sequences=True if idx != len(n_neurons_hl)-1 else False\n",
    "#     print(return_sequences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P/ n_neurons_hl = [n_neurons_hidden_layer_1, ... , n_neurons_hidden_layer_N], return_sequences=True, \n",
    "# exceto para a N-ésima camada (última).\n",
    "\n",
    "def build_model(X_train_, n_neurons_hl, activation): \n",
    "    model = tf.keras.Sequential([ \n",
    "        tf.keras.layers.GRU(\n",
    "            units=n_neurons_hl[0], \n",
    "            activation=activation,\n",
    "            input_shape=[*X_train_.shape[1:]],\n",
    "            return_sequences=(True if len(n_neurons_hl) > 1 else False)\n",
    "        ),\n",
    "        *[\n",
    "            tf.keras.layers.GRU(\n",
    "                units=n_neurons, \n",
    "                activation=activation,\n",
    "                return_sequences=(True if idx != len(n_neurons_hl[1:])-1 else False) \n",
    "            ) for idx, n_neurons in enumerate(n_neurons_hl[1:])\n",
    "        ],\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience, X_train_, y_train_, max_epochs, monitor_metric): \n",
    "    model.compile(loss=tf.losses.MeanAbsoluteError(),\n",
    "                    optimizer=tf.optimizers.Adam(),\n",
    "                    metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=patience, \n",
    "            mode='min',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=best_file_ann, \n",
    "            monitor=monitor_metric,\n",
    "            verbose=0, # True  \n",
    "            save_best_only=True\n",
    "        )  \n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_,\n",
    "        y_train_,\n",
    "        epochs=max_epochs,\n",
    "        verbose=0, #True\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # model.save(file_ann) # salva o modelo final do treinamento\n",
    "    model = tf.keras.models.load_model(best_file_ann) # usamos o best_model do treinamento\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_run(n, f, run_name, model_params, train_params, df):\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # Logging params\n",
    "        mlflow.log_param(\"model\", model_)\n",
    "        mlflow.log_param(\"label\", label)\n",
    "        mlflow.log_param(\"n_so_retro\", n)\n",
    "        mlflow.log_param(\"f_so_pred\", f)\n",
    "        mlflow.log_param(\"seed\", RANDOM_SEED)\n",
    "        for key, value in model_params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "        for key, value in train_params.items():\n",
    "            mlflow.log_param(key, value)\n",
    "\n",
    "        # Logging run_name as a tag\n",
    "        mlflow.set_tag(\"run_name\", run_name)\n",
    "\n",
    "        # Mount dataset \n",
    "        df_ = dataset_constructor(df.copy(), f)\n",
    "\n",
    "        new_df, scaler_y = scaling_data(df_, f)\n",
    "\n",
    "        # Prepare X and y data and apply train_test_split\n",
    "        X, y = split_sequences(new_df.values, n, f)\n",
    "        \n",
    "        X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "        # Mount model \n",
    "        model = build_model(X_train_[:,:,1:], \n",
    "                            model_params['n_neurons_hl'], \n",
    "                            model_params['activation'])\n",
    "\n",
    "\n",
    "        model = train_model(model, \n",
    "                    train_params['patience'], \n",
    "                    X_train_[:,:,1:].astype('float32'), \n",
    "                    y_train_[:,1].astype('float32'), \n",
    "                    train_params['max_epochs'],\n",
    "                    train_params['monitor_metric']) # retorna o best_model\n",
    "\n",
    "\n",
    "        y_pred_ = model.predict(X_test_[:,:,1:].astype('float32'))\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_)\n",
    "\n",
    "        y_test = scaler_y.inverse_transform(y_test_[:,1].reshape(-1, 1))\n",
    "\n",
    "        rmse, mae, r2, corr, kge_, kge_r, kge_alpha, kge_beta = evaluation_metrics(y_test, y_pred)\n",
    "        \n",
    "        # signature = mlflow.models.signature.infer_signature(X_test, y_test_)\n",
    "        # mlflow.sklearn.log_model(model, \"sk_models\", signature=signature)\n",
    "\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2\", r2)  \n",
    "        mlflow.log_metric(\"corr\", corr)\n",
    "        # mlflow.log_metric(\"nse\", nse_)\n",
    "        mlflow.log_metric(\"kge\", kge_)\n",
    "        mlflow.log_metric(\"kge_r\", kge_r)\n",
    "        mlflow.log_metric(\"kge_alpha\", kge_alpha)\n",
    "        mlflow.log_metric(\"kge_beta\", kge_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Número de Semanas Operativas Retroativas a serem utilizadas no Treinamento dos Algoritmos. min(n)=1\n",
    "n = 3\n",
    "\n",
    "## Número da Semana Operativa Futura da Vazão a ser prevista pelos Modelos. min(f)=1\n",
    "f = 1\n",
    "\n",
    "run_name = f'single_{model_}_n={n}_f={f}_'\n",
    "\n",
    "model_params = {\n",
    "    'n_neurons_hl' : [50,60], # [n_neurons_hidden_layer_1, ... , n_neurons_hidden_layer_N]\n",
    "    'activation' : 'relu'\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'patience' : 10,\n",
    "    'max_epochs' : 500,\n",
    "    'monitor_metric' : 'val_mean_absolute_error' \n",
    "}\n",
    "\n",
    "df = pd.read_csv(f'{path_datasets}/{dataset}', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run(n, f, run_name, model_params, train_params, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.search_runs(filter_string=f\"tags.run_name='{run_name}'\")#.iloc[0].status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Número de Semanas Operativas Retroativas a serem utilizadas no Treinamento dos Algoritmos\n",
    "n_range = range(1,8+1)\n",
    "\n",
    "## Número da Semana Operativa Futura da Vazão a ser prevista pelos Modelos. min(f)=1\n",
    "f_range = range(1,8+1)\n",
    "\n",
    "# Podem variar\n",
    "model_params = {}\n",
    "\n",
    "n_neurons_hl1_range = (40,50,60) \n",
    "n_neurons_hl2_range = (40,50,60)  \n",
    "n_neurons_hl3_range = (40,50,60) # fazer teste p/ 3 camadas\n",
    "\n",
    "activation_range = ('relu', 'sigmoid') \n",
    "\n",
    "# São fixos\n",
    "train_params = {\n",
    "    'patience' : 15,\n",
    "    'max_epochs' : 500,\n",
    "    'monitor_metric' : 'val_mean_absolute_error' # para salvar o best_model\n",
    "}\n",
    "\n",
    "# não mecher ainda na feature_range=(-1, 1) -> mais pra tanh LSTM, GRU\n",
    "\n",
    "df = pd.read_csv(f'{path_datasets}/{dataset}', index_col='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = '1hl'\n",
    "\n",
    "for n in n_range:\n",
    "    for f in f_range:\n",
    "        for n_neurons_hl1 in n_neurons_hl1_range:\n",
    "            for activation in activation_range:\n",
    "                \n",
    "                run_name = (\n",
    "                    f\"{model_}_n={n}_f={f}_\"\n",
    "                    f\"{n_neurons_hl1}_\"\n",
    "                    f\"{activation}_\"\n",
    "                    f\"{n_hidden_layers}_\"\n",
    "                )\n",
    "\n",
    "                # Check if the run_name already exists\n",
    "                existing_runs = mlflow.search_runs(filter_string=f\"tags.run_name='{run_name}'\")\n",
    "                if not existing_runs.empty:\n",
    "                    last_run = existing_runs.iloc[0]  # Check the most recent run\n",
    "                    if last_run[\"status\"] == \"FAILED\":\n",
    "                        print(f\"Run '{run_name}' failed previously. Re-running.\")\n",
    "                    else:\n",
    "                        print(f\"Run '{run_name}' already exists. Skipping iteration.\")\n",
    "                        continue\n",
    "                \n",
    "                model_params['n_neurons_hl'] = [n_neurons_hl1]\n",
    "                model_params['activation'] = activation\n",
    "\n",
    "                mlflow_run(n, f, run_name, model_params, train_params, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = '2hl'\n",
    "\n",
    "for n in n_range:\n",
    "    for f in f_range:\n",
    "        for n_neurons_hl1 in n_neurons_hl1_range:\n",
    "            for n_neurons_hl2 in n_neurons_hl2_range:\n",
    "                for activation in activation_range:\n",
    "                    \n",
    "                    run_name = (\n",
    "                        f\"{model_}_n={n}_f={f}_\"\n",
    "                        f\"{n_neurons_hl1}_\"\n",
    "                        f\"{n_neurons_hl2}_\"\n",
    "                        f\"{activation}_\"\n",
    "                        f\"{n_hidden_layers}_\"\n",
    "                    )\n",
    "\n",
    "                    # Check if the run_name already exists\n",
    "                    existing_runs = mlflow.search_runs(filter_string=f\"tags.run_name='{run_name}'\")\n",
    "                    if not existing_runs.empty:\n",
    "                        last_run = existing_runs.iloc[0]  # Check the most recent run\n",
    "                        if last_run[\"status\"] == \"FAILED\":\n",
    "                            print(f\"Run '{run_name}' failed previously. Re-running.\")\n",
    "                        else:\n",
    "                            print(f\"Run '{run_name}' already exists. Skipping iteration.\")\n",
    "                            continue\n",
    "                    \n",
    "                    model_params['n_neurons_hl'] = [n_neurons_hl1,n_neurons_hl2]\n",
    "                    model_params['activation'] = activation\n",
    "\n",
    "                    mlflow_run(n, f, run_name, model_params, train_params, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = '3hl'\n",
    "\n",
    "for n in n_range:\n",
    "    for f in f_range:\n",
    "        for n_neurons_hl1 in n_neurons_hl1_range:\n",
    "            for n_neurons_hl2 in n_neurons_hl2_range:\n",
    "                for n_neurons_hl3 in n_neurons_hl3_range:\n",
    "                    for activation in activation_range:\n",
    "                        \n",
    "                        run_name = (\n",
    "                            f\"{model_}_n={n}_f={f}_\"\n",
    "                            f\"{n_neurons_hl1}_\"\n",
    "                            f\"{n_neurons_hl2}_\"\n",
    "                            f\"{n_neurons_hl3}_\"\n",
    "                            f\"{activation}_\"\n",
    "                            f\"{n_hidden_layers}_\"\n",
    "                        )\n",
    "\n",
    "                        # Check if the run_name already exists\n",
    "                        existing_runs = mlflow.search_runs(filter_string=f\"tags.run_name='{run_name}'\")\n",
    "                        if not existing_runs.empty:\n",
    "                            last_run = existing_runs.iloc[0]  # Check the most recent run\n",
    "                            if last_run[\"status\"] == \"FAILED\":\n",
    "                                print(f\"Run '{run_name}' failed previously. Re-running.\")\n",
    "                            else:\n",
    "                                print(f\"Run '{run_name}' already exists. Skipping iteration.\")\n",
    "                                continue\n",
    "                        \n",
    "                        model_params['n_neurons_hl'] = [n_neurons_hl1,n_neurons_hl2,n_neurons_hl3]\n",
    "                        model_params['activation'] = activation\n",
    "\n",
    "                        mlflow_run(n, f, run_name, model_params, train_params, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".euros",
   "language": "python",
   "name": ".euros"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

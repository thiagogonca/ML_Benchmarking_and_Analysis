{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do dataset para treino da LSTM (RNN) em séries temporais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=21\n",
    "\n",
    "model = \"lstm\"\n",
    "src_type = \"regular\"\n",
    "\n",
    "dir_results = f\"../../data/results/{src_type}\"\n",
    "dir_figures = f\"{dir_results}/figures/{model}\"\n",
    "\n",
    "if not os.path.exists(dir_figures):\n",
    "    os.makedirs(dir_figures)\n",
    "\n",
    "path_datasets = \"../../data/datasets\"\n",
    "dataset = \"Itaipu_POC_VAZAO_V3.csv\"\n",
    "\n",
    "## Número de Semanas Operativas Retroativas a serem utilizadas no Treinamento dos Algoritmos. min(n)=1\n",
    "n = 6\n",
    "\n",
    "## Número da Semana Operativa Futura da Vazão a ser prevista pelos Modelos. min(f)=1\n",
    "f = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{path_datasets}/{dataset}', index_col='time')\n",
    "# df.reset_index().drop(columns='time')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = df.index\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df[f'bacia_prec_sum_shift_f={f}'] = df['bacia_prec_sum'].shift(-f)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Columns to scale for X and y\n",
    "columns_to_scale_X = [f'bacia_prec_sum_shift_f={f}'] # 'bacia_prec_sum',\n",
    "columns_to_scale_y = ['vazao_itaipu']\n",
    "\n",
    "# Fit scalers on the selected columns and transform\n",
    "scaled_data_X = scaler_X.fit_transform(df[columns_to_scale_X])\n",
    "scaled_data_y = scaler_y.fit_transform(df[columns_to_scale_y])\n",
    "\n",
    "# Create DataFrame with scaled data\n",
    "scaled_X = pd.DataFrame(scaled_data_X, columns=columns_to_scale_X)\n",
    "scaled_y = pd.DataFrame(scaled_data_y, columns=columns_to_scale_y)\n",
    "\n",
    "# Concatenate scaled columns to the original DataFrame\n",
    "new_df = pd.concat([df.time, scaled_X, scaled_y], axis=1)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.values\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão dos datasets em séries temporais e treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps, f_pred):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+(f_pred-1) >= len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix+(f_pred-1), [0,2]]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_df)#Divide uma sequencia multivariável em amostras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Para debugar a função split_sequences\n",
    "sequences = new_df\n",
    "i=0\n",
    "end_ix=i+n\n",
    "\n",
    "print(end_ix, end_ix+(f-1))\n",
    "\n",
    "sequences[i:end_ix, :], sequences[end_ix+(f-1), [0,2]] \n",
    "\n",
    "# Na sequência do target (y)\n",
    "#\n",
    "# end_ix -> 1 S.O futura\n",
    "#\n",
    "# end_ix+1 -> 2 S.O futuras\n",
    "#\n",
    "# end_ix+2 -> 3 S.O futuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_sequences(new_df, n, f)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 0\n",
    "X[seq], y[seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train_.shape, X_test_.shape, y_train_.shape, y_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 0\n",
    "X_train_[:,:,1:][seq], y_train_[:,1][seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime('%Y%m%d') # _%Hh%M\n",
    "modelo_numerico = 'so_prev' # previsão para a semana operacional seguinte\n",
    "\n",
    "dir_rna = f'{dir_results}/rna/{modelo_numerico}_{now}'\n",
    "if not os.path.exists(dir_rna):\n",
    "    os.makedirs(dir_rna)\n",
    "\n",
    "file_ann = f'{dir_rna}/ann_{modelo_numerico}.h5' \n",
    "best_file_ann = f'{dir_rna}/best_ann_{modelo_numerico}.h5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_metric = 'val_mean_absolute_error'\n",
    "patience=15\n",
    "n_neurons = 256\n",
    "max_epochs = 500\n",
    "n_hidden_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_[:,:,1:].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train_, n_neurons_hl, activation): \n",
    "    model = tf.keras.Sequential([ \n",
    "        tf.keras.layers.LSTM(\n",
    "            units=n_neurons_hl[0], \n",
    "            activation=activation,\n",
    "            input_shape=[*X_train_.shape[1:]],\n",
    "            return_sequences=(True if len(n_neurons_hl) > 1 else False)\n",
    "        ),\n",
    "        *[\n",
    "            tf.keras.layers.LSTM(\n",
    "                units=n_neurons, \n",
    "                activation=activation,\n",
    "                return_sequences=(True if idx != len(n_neurons_hl[1:])-1 else False) \n",
    "            ) for idx, n_neurons in enumerate(n_neurons_hl[1:])\n",
    "        ],\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train_[:,:,1:], [50,60], 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=patience, \n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=best_file_ann, \n",
    "        monitor=monitor_metric,\n",
    "        verbose=True, \n",
    "        save_best_only=True\n",
    "        )  \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_[:,:,1:].astype('float32'),\n",
    "    y_train_[:,1].astype('float32'),\n",
    "    epochs=max_epochs,\n",
    "    verbose=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(file_ann) # salva o modelo atual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importamos o modelo que melhor performou em 'monitor_metric' durante o treinamento para analisar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(best_file_ann) # importamos o modelo que melhor performou em 'monitor_metric' durante o treinamento para analisar\n",
    "# model = tf.keras.models.load_model(file_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(1, len(history.history['loss']) + 1)),\n",
    "                         y=history.history['loss'],\n",
    "                         mode='lines',\n",
    "                         name='Train Loss'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_loss']) + 1)),\n",
    "                         y=history.history['val_loss'],\n",
    "                         mode='lines',\n",
    "                         name='Validation Loss'))\n",
    "\n",
    "fig.update_layout(title='Training and Validation Loss',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Loss',\n",
    "                  legend=dict(x=0, y=1, traceorder='normal'),\n",
    "                  width=900, height=600)\n",
    "\n",
    "fig.write_image(f\"{dir_figures}/rna_training_validation_loss_plot.png\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retoma a transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = model.predict(X_test_[:,:,1:].astype('float32'))#.astype('float32')\n",
    "y_pred = scaler_y.inverse_transform(y_pred_)#.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = scaler_y.inverse_transform(y_test_[:,1].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = y_test.reshape(-1, y_test.shape[-1])\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test.T, y_pred.T)[0, 1]\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    columns=['MAE', 'MSE', 'RMSE', 'R2', 'Corr'],\n",
    "    index=['Decision Tree']\n",
    ")\n",
    "\n",
    "metrics_df['MAE'] = mae\n",
    "metrics_df['MSE'] = mse\n",
    "metrics_df['RMSE'] = rmse\n",
    "metrics_df['R2'] = r2\n",
    "metrics_df['Corr'] = corr\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test.ravel(),\n",
    "        y=y_pred.ravel(),\n",
    "        mode='markers',\n",
    "        marker=dict(color='blue', opacity=0.5, line=dict(color='black', width=1)),\n",
    "        name='Measured vs Predicted'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[y_pred.min(), y_pred.max()],\n",
    "        y=[y_pred.min(), y_pred.max()],\n",
    "        mode='lines',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        name='Identity Line'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Measured vs Predicted',\n",
    "    xaxis=dict(title='y_true'),\n",
    "    yaxis=dict(title='y_pred'),\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.write_image(f\"{dir_figures}/scattered_measured_vs_predicted_plot.png\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df.time.values,\n",
    "        y=df.vazao_itaipu.values, # vazão observada\n",
    "        mode='lines',\n",
    "        name='Vazão observada',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test_[:,0],\n",
    "        y=y_pred.ravel(), # vazão prevista\n",
    "        mode='markers',\n",
    "        name='Forecast',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title=f'Predição - Itaipu')\n",
    "\n",
    "fig.write_image(f\"{dir_figures}/history_measured_vs_predicted_plot.png\", width=1400, scale=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_importance(seq, model):\n",
    "#     seq = tf.Variable(seq[np.newaxis,:,:], dtype=tf.float32)\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(seq)\n",
    "#     grads = tape.gradient(predictions, seq)\n",
    "#     grads = tf.reduce_mean(grads, axis=1).numpy()[0]\n",
    "    \n",
    "#     return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_importance(X_train[0,:,1:], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = []\n",
    "# for i in range(0, X_train.shape[0]):\n",
    "#     importances.append(gradient_importance(X_train[i,:,1:], model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance = np.mean(np.array(importances), axis=0)\n",
    "# importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6,4))\n",
    "# plt.title('Importância das Variáveis')\n",
    "\n",
    "# plt.bar(df_ts.columns.values[:importance.shape[0]],importance.tolist())\n",
    "\n",
    "# plt.savefig(f\"{dir_figures}/feature_importance.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalização dos dados de treino/teste\n",
    "\n",
    "# # Reshape the data to 2D\n",
    "# X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "# X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "# y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])\n",
    "# y_test_reshaped = y_test.reshape(-1, y_test.shape[-1])\n",
    "\n",
    "# # Initialize MinMaxScaler\n",
    "# scaler_X = MinMaxScaler()\n",
    "# scaler_y = MinMaxScaler()\n",
    "\n",
    "# # Fit and transform on the training data (ignoring dates) # _ stands for normalized data\n",
    "# X_train_ = scaler_X.fit_transform(X_train_reshaped[:,1:])\n",
    "# y_train_ = scaler_y.fit_transform(y_train_reshaped[:,1:])\n",
    "\n",
    "# # Transform the test data (ignoring dates)\n",
    "# X_test_ = scaler_X.transform(X_test_reshaped[:,1:])\n",
    "# y_test_ = scaler_y.transform(y_test_reshaped[:,1:])\n",
    "\n",
    "# # Reshape back to the original shape (ignoring dates)\n",
    "# X_train_ = X_train_.reshape(X_train[:,:,1:].shape).astype('float32')\n",
    "# X_test_ = X_test_.reshape(X_test[:,:,1:].shape).astype('float32')\n",
    "# y_train_ = y_train_.reshape(y_train[:,1:].shape).astype('float32')\n",
    "# y_test_ = y_test_.reshape(y_test[:,1:].shape).astype('float32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".euros",
   "language": "python",
   "name": ".euros"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
